<!DOCTYPE html>
<html lang="en-us">
    <head>
        

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        
<meta name="description" content="Personal blog about Neural Networks, NLP and Software Engineering.">
<meta name="keywords" content="NLP, neural networks, pytorch, telegram, engineering, ML, fasttext, simaes networks, BERT, transformer, word2vec, deploy, allennlp, blog">


<script type="application/ld+json">
  {
      "@context" : "http://schema.org",
      "@type" : "BlogPosting",
      "mainEntityOfPage": {
           "@type": "WebPage",
           "@id": "https:\/\/blog.vasnetsov.com\/"
      },
      "articleSection" : "posts",
      "name" : "Allennlp Seq2seq",
      "headline" : "Allennlp Seq2seq",
      "description" : "AllenNLP - customizable PyTorch framework with proper OOP, tons of model examples and utils.",
      "inLanguage" : "en-US",
      "author" : "Andrey Vasnetsov",
      "creator" : "Andrey Vasnetsov",
      "publisher": "Andrey Vasnetsov",
      "accountablePerson" : "Andrey Vasnetsov",
      "copyrightHolder" : "Andrey Vasnetsov",
      "copyrightYear" : "2018",
      "datePublished": "2018-12-28 01:48:45 \u002b0300 \u002b0300",
      "dateModified" : "2018-12-28 01:48:45 \u002b0300 \u002b0300",
      "url" : "https:\/\/blog.vasnetsov.com\/posts\/allennlp-seq2seq\/",
      "wordCount" : "483",
      "keywords" : [ "NLP", "neural networks", "pytorch", "telegram", "engineering", "ML", "fasttext", "simaes networks", "BERT", "transformer", "word2vec", "deploy", "allennlp", "Blog" ]
  }
</script>

        <title>Allennlp Seq2seq</title>
        
        <style>

    html body {
        font-family: 'Raleway', sans-serif;
        background-color: white;
    }

    :root {
        --accent: red;
        --border-width:  5px ;
    }

</style>


<link rel="stylesheet" href="https://blog.vasnetsov.com/css/main.css">





<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Raleway">


 <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/monokailight.min.css"> 


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
 

    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/scala.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/json.min.js"></script>
    
    <script>hljs.initHighlightingOnLoad();</script>






<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>


<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>


<script>$(document).on('click', function() { $('.collapse').collapse('hide'); })</script>
 <meta name="generator" content="Hugo 0.72.0" />
        

        
            <script async src="https://www.googletagmanager.com/gtag/js?id=UA-56301881-3"></script>
            <script>
              window.dataLayer = window.dataLayer || [];
              function gtag(){dataLayer.push(arguments)};
              gtag('js', new Date());
              gtag('config', 'UA-56301881-3');
            </script>
        

        
            <meta name="yandex-verification" content="6c7b43afecb1426c" />
        

        
            <script type="text/x-mathjax-config">
                MathJax.Hub.Config({
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    processEscapes: true
                }
                });
            </script>
            <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        

        

    </head>

    <body>
        
        <nav class="navbar navbar-default navbar-fixed-top">
            <div class="container">
                <div class="navbar-header">
                    <a class="navbar-brand visible-xs" href="#">Allennlp Seq2seq</a>
                    <button class="navbar-toggle" data-target=".navbar-collapse" data-toggle="collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                </div>
                <div class="collapse navbar-collapse">
                    
                        <ul class="nav navbar-nav">
                            
                                <li><a href="https://blog.vasnetsov.com/">Home</a></li>
                            
                                <li><a href="https://blog.vasnetsov.com/andrey_vasnetsov_cv.pdf">CV</a></li>
                            
                                <li><a href="https://blog.vasnetsov.com/posts/">Posts</a></li>
                            
                        </ul>
                    
                    
                        <ul class="nav navbar-nav navbar-right">
                            
                                <li class="navbar-icon"><a href="mailto:andrey&#43;blog@vasnetsov.com"><i class="fa fa-envelope-o"></i></a></li>
                            
                                <li class="navbar-icon"><a href="https://github.com/generall"><i class="fa fa-github"></i></a></li>
                            
                                <li class="navbar-icon"><a href="https://t.me/neural_network_engineering"><i class="fa fa-telegram"></i></a></li>
                            
                        </ul>
                    
                </div>
            </div>
        </nav>


<main>
    
        <div>
            <h2>Allennlp Seq2seq</h2>
            <h5>December 28, 2018</h5>
            

        </div>

        <div align="start" class="content"><p>Let&rsquo;s continue to dive into Question Answering. Last time we <a href="https://github.com/generall/allennlp_sandbox/blob/master/SyntheticQA.ipynb">have generated</a> several variants of synthetic sequences, from which we need to extract &ldquo;answers&rdquo;. Each sequence type has each own pattern, and we want a neural network to find it.
In a most general sense, this task looks like sequence transformation - Seq2seq, similar to <a href="http://opennmt.net/">NMT</a>.
In this post, I will describe how to implement a simple Seq2seq network with <a href="https://github.com/allenai/allennlp">AllenNLP framework</a>.</p>
<p>AllenNLP library includes components that standardize and simplify the creation of neural networks for text processing.
Its developers conducted a great work decomposing variety of NLP tasks into separate blocks.
It allowed to implement a set of universal pipeline components, suitable for reuse.
Implemented components could be used directly from code or for creating configs.</p>
<p>I have created a <a href="https://github.com/generall/allennlp_sandbox">repository</a> for my experiments. It contains a simple config file along with some accessory files. Let&rsquo;s take a <a href="https://github.com/generall/allennlp_sandbox/blob/master/allen_test_conf.json">look</a>.</p>
<p>One of the main configuration parameters is a <code>model</code>.  The model determines what happens to the data and the network during training and forecasting. The model parameter itself is a class that derives from <code>allennlp.models.model.Model</code> and implements the <code>forward</code> method.
We will use <a href="https://github.com/generall/allennlp_sandbox/blob/master/allen_test_conf.json#L12">simple_seq2seq</a> model which implements a basic sequence transformation scheme.</p>
<p><img src="https://blog.vasnetsov.com/img/seq2seq.png" alt="seq2seq model"></p>
<p>In classical seq2seq the source sequence is transformed by Encoder into representation, which is then read by Decoder to generate the target sequence.
<code>simple_seq2seq</code> module implements only Decoder. The Encoder should be implemented in other class, passed as a <a href="https://github.com/generall/allennlp_sandbox/blob/master/allen_test_conf.json#L23">model parameter</a>.
We will use the simplest encoder option - LSTM.</p>
<p>Here are some other useful model parameters:</p>
<ul>
<li><code>source_embedder</code> - This class assigns a pre-trained vector to each input token. We have no pre-trained vectors for synthetic data so we will use random vectors. We will also make them untrainable to prevent overfitting.</li>
<li><code>attention</code> - <a href="http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">attention function</a>, used on each decoding step. Attention vector is concatenated with decoder state.</li>
<li><code>beam_size</code> - the number of variants, generated by <a href="https://hackernoon.com/beam-search-a-search-strategy-5d92fb7817f">beam search</a> during decoding.</li>
<li><code>scheduled_sampling_ratio</code> - defines whether to use real or generated elements as a previous element during decoding.</li>
</ul>
<p>Then we save our dataset so that the <code>seq2seq</code> <a href="https://github.com/generall/allennlp_sandbox/blob/master/allen_test_conf.json#L2">dataset reader</a>, implemented in AllenNLP, could work with it. Now we can launch training with a single command <code>allennlp train config.json</code> and observe training statistics on a Tensorboard.
A trained model could be easily used from Python, here is an <a href="https://github.com/generall/allennlp_sandbox/blob/master/predictor.py">example</a>.</p>
<p>It should be noticed, that model is quickly overfitting on a synthetic data, so I generated a lot of it.</p>
<p>Unfortunately, AllenNLP seq2seq module is still under construction. It can&rsquo;t handle all existing variants of seq2seq models. For example, you can&rsquo;t implement Attention Transformer architecture from the article <a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a>. Attention Transformer requires a custom decoder, but it is hardcoded in <code>simple_seq2seq</code>. If you want to contribute AllenNLP seq2seq model, please, take a look at <a href="https://github.com/allenai/allennlp/issues/2097">this issue</a>. If you leave your reaction, it will help to focus AllenNLP developers attention on it.</p>
</div>

        
        
        

        
        
    
</main>

        <footer>
            <p class="copyright text-muted">Â© All rights reserved. Powered by <a href="https://gohugo.io">Hugo</a> and <a href="https://github.com/calintat/minimal">Minimal</a>.</p>
        </footer>

        

        
    </body>

</html>

