<!DOCTYPE html>
<html lang="en-us">
    <head>
        

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        
<meta name="description" content="Personal blog about Neural Networks, NLP and Software Engineering.">
<meta name="keywords" content="NLP, neural networks, pytorch, telegram, engineering, ML, fasttext, simaes networks, BERT, transformer, word2vec, deploy, allennlp, blog">


<script type="application/ld+json">
  {
      "@context" : "http://schema.org",
      "@type" : "BlogPosting",
      "mainEntityOfPage": {
           "@type": "WebPage",
           "@id": "https:\/\/comprehension.ml\/"
      },
      "articleSection" : "posts",
      "name" : "Shrincking Fasttext",
      "headline" : "Shrincking Fasttext",
      "description" : "Shrinking fastText embeddings so that it fits Google Colab",
      "inLanguage" : "en-US",
      "author" : "Andrey Vasnetsov",
      "creator" : "Andrey Vasnetsov",
      "publisher": "Andrey Vasnetsov",
      "accountablePerson" : "Andrey Vasnetsov",
      "copyrightHolder" : "Andrey Vasnetsov",
      "copyrightYear" : "2019",
      "datePublished": "2019-04-28 01:47:20 \x2b0300 MSK",
      "dateModified" : "2019-04-28 01:47:20 \x2b0300 MSK",
      "url" : "https:\/\/comprehension.ml\/posts\/shrincking-fasttext\/",
      "wordCount" : "804",
      "keywords" : [ "NLP", "neural networks", "pytorch", "telegram", "engineering", "ML", "fasttext", "simaes networks", "BERT", "transformer", "word2vec", "deploy", "allennlp", "Blog" ]
  }
</script>

        <title>Shrincking Fasttext</title>
        
        <style>

    html body {
        font-family: 'Raleway', sans-serif;
        background-color: white;
    }

    :root {
        --accent: red;
        --border-width:  5px ;
    }

</style>


<link rel="stylesheet" href="https://comprehension.ml/css/main.css">





<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Raleway">


 <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/monokailight.min.css"> 


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
 

    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/scala.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/json.min.js"></script>
    
    <script>hljs.initHighlightingOnLoad();</script>






<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>


<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>


<script>$(document).on('click', function() { $('.collapse').collapse('hide'); })</script>
 <meta name="generator" content="Hugo 0.60.0" />
        

        
            <script async src="https://www.googletagmanager.com/gtag/js?id=UA-56301881-3"></script>
            <script>
              window.dataLayer = window.dataLayer || [];
              function gtag(){dataLayer.push(arguments)};
              gtag('js', new Date());
              gtag('config', 'UA-56301881-3');
            </script>
        

        
            <script type="text/x-mathjax-config">
                MathJax.Hub.Config({
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    processEscapes: true
                }
                });
            </script>
            <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        

        

    </head>

    <body>
        
        <nav class="navbar navbar-default navbar-fixed-top">
            <div class="container">
                <div class="navbar-header">
                    <a class="navbar-brand visible-xs" href="#">Shrincking Fasttext</a>
                    <button class="navbar-toggle" data-target=".navbar-collapse" data-toggle="collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                </div>
                <div class="collapse navbar-collapse">
                    
                        <ul class="nav navbar-nav">
                            
                                <li><a href="https://comprehension.ml/">Home</a></li>
                            
                                <li><a href="https://comprehension.ml/andrey_vasnetsov_cv.pdf">CV</a></li>
                            
                                <li><a href="https://comprehension.ml/posts/">Posts</a></li>
                            
                        </ul>
                    
                    
                        <ul class="nav navbar-nav navbar-right">
                            
                                <li class="navbar-icon"><a href="mailto:vasnetsov93&#43;blog@gmail.com"><i class="fa fa-envelope-o"></i></a></li>
                            
                                <li class="navbar-icon"><a href="https://github.com/generall"><i class="fa fa-github"></i></a></li>
                            
                                <li class="navbar-icon"><a href="https://t.me/neural_network_engineering"><i class="fa fa-telegram"></i></a></li>
                            
                        </ul>
                    
                </div>
            </div>
        </nav>


<main>
    
        <div>
            <h2>Shrincking Fasttext</h2>
            <h5>April 28, 2019</h5>
            

        </div>

        <div align="start" class="content"><p><img src="https://comprehension.ml/img/fasttext_colab.png" alt="OOM"></p>
<p><a href="https://fasttext.cc/docs/en/crawl-vectors.html">Pretrained</a> fastText embeddings are great.
They were trained on a many languages, carry subword information, support OOV words.</p>
<p>But their main disadvantage is the size. Even compressed version of the binary model takes 5.4Gb.
This fact makes it impossible to use pretrained models on a laptop or a small VM instances.</p>
<p>Being loaded into RAM, this model takes even more memory ~ 16Gb.
So you can't use it directly with Google Colab, which only gives you 12 GB of RAM.</p>
<p>There are two main reasons why binary models weight so much.
The first one is that binary model carries not only weights for words and n-grams, but also weights for translating vectors back to words.
These vectors are also called negative vectors and are used for additional training of the model.</p>
<p>The second reason is that word and n-gram matrices are very large, they contain 2M vectors each.
This is more than enough for most practical applications.</p>
<p>Let's deal with both of this problems.</p>
<p>The easiest part is to just drop trainables. One way to do this is to convert the native fastText format into <a href="https://radimrehurek.com/gensim/">Gensim</a> representation.</p>
<pre><code>from gensim.models import FastText

# Original fasttext embeddings from https://fasttext.cc/
ft = FastText.load_fasttext_format(&quot;fasttext_embedding.bin&quot;) 

# we are not saving trainables here 
ft.wv.save('fasttext_gensim.model') 
</code></pre><p>That is easy. Now instead of 16Gb of RAM, our model will take only 10Gb.</p>
<p>Let's go further.</p>
<p>There are two embedding matrices in the fastText model: vocab matrix and n-gram matrix.
The first one is simple, it just holds the embedding vector for each word in the vocabulary.
Vocabulary is also presented and is sorted by frequency, so the only thing we need to do is to take the first N rows from this matrix and remove infrequent words from the vocabulary. You may find a reference to my implementation at the bottom of this article.</p>
<p>The second matrix is more tricky.</p>
<p>It does not hold a direct mapping of n-grams to vectors inside this matrix.
Instead of it, the fastText model performs hashing trick: It calculates n-gram hashes and maps its remainder of the division by matrix size.
This trick allows having some vector representation for any n-gram with a little collision probability.</p>
<p>So to shrink this matrix we will need to re-map old indexes to new ones.
And here comes two things we need to decide:</p>
<ul>
<li>How to resolve collisions.</li>
<li>How to choose a new matrix size.</li>
</ul>
<p>The answer to the first question is more practical than theoretical.
I have just tested several variants and came up with the decision to use a simple weighted average of collided n-grams, where the weights are derived from n-gram frequency.</p>
<p>The second answer may be not so intuitive like the first one.
At first glance, you might think that the larger the new matrix is, the more information we preserve. But it is not true.</p>
<p>The loss of information depends not only on the total number of collisions but also on the range of variants this collision has to be resolved on.</p>
<p>Imagine we have 10 collisions among 2 n-grams each and a single collision among 20 n-grams.
In the first case, we can save half of the information and only 1/20 in the second one.</p>
<p>This means that we are also interested in the uniform distribution of collisions.
We can achieve this uniformity if the size of the smaller matrix has a larger <a href="https://en.wikipedia.org/wiki/Greatest_common_divisor">GCD</a> with the original size.
In this case, the amount of n-grams candidates for a single id will not exceed <code>old_size / GCD(old_size, new_size)</code></p>
<p>Thereby it would be efficient to compress 2M matrix into 1M, 500K or 250K.</p>
<p>To ensure that new n-gram matrix is not broken, we may need to estimate quality.
One way to do it is to measure the similarity between vocab vectors of the original model and reconstructed from n-grams vectors of a new model.</p>
<div class="highlight"><pre style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#111">test_vocab_size</span> <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span><span style="color:#111">_000</span>
<span style="color:#111">test_vocab</span> <span style="color:#f92672">=</span> <span style="color:#111">sorted_vocab</span><span style="color:#111">[</span><span style="color:#111">new_vocab_size</span><span style="color:#111">:</span><span style="color:#111">new_vocab_size</span> <span style="color:#f92672">+</span> <span style="color:#111">test_vocab_size</span><span style="color:#111">]</span>
<span style="color:#111">sims</span> <span style="color:#f92672">=</span> <span style="color:#111">[</span><span style="color:#111">]</span>
<span style="color:#00a8c8">for</span> <span style="color:#111">test_word</span><span style="color:#111">,</span> <span style="color:#111">_</span> <span style="color:#f92672">in</span> <span style="color:#111">test_vocab</span><span style="color:#111">:</span>
    <span style="color:#111">sim</span> <span style="color:#f92672">=</span> <span style="color:#111">ft</span><span style="color:#f92672">.</span><span style="color:#111">cosine_similarities</span><span style="color:#111">(</span>
      <span style="color:#111">ft</span><span style="color:#f92672">.</span><span style="color:#111">get_vector</span><span style="color:#111">(</span><span style="color:#111">test_word</span><span style="color:#111">)</span><span style="color:#111">,</span>
      <span style="color:#111">[</span><span style="color:#111">new_ft</span><span style="color:#f92672">.</span><span style="color:#111">get_vector</span><span style="color:#111">(</span><span style="color:#111">test_word</span><span style="color:#111">)</span><span style="color:#111">]</span>
    <span style="color:#111">)</span>
    <span style="color:#00a8c8">if</span> <span style="color:#f92672">not</span> <span style="color:#111">np</span><span style="color:#f92672">.</span><span style="color:#111">isnan</span><span style="color:#111">(</span><span style="color:#111">sim</span><span style="color:#111">)</span><span style="color:#111">:</span>
        <span style="color:#111">sims</span><span style="color:#f92672">.</span><span style="color:#111">append</span><span style="color:#111">(</span><span style="color:#111">sim</span><span style="color:#111">)</span>

<span style="color:#00a8c8">print</span><span style="color:#111">(</span><span style="color:#d88200"></span><span style="color:#d88200">&#34;</span><span style="color:#d88200">Similarity:</span><span style="color:#d88200">&#34;</span><span style="color:#111">,</span> <span style="color:#111">np</span><span style="color:#f92672">.</span><span style="color:#111">sum</span><span style="color:#111">(</span><span style="color:#111">sims</span><span style="color:#111">)</span> <span style="color:#f92672">/</span> <span style="color:#111">test_vocab_size</span><span style="color:#111">)</span>
</code></pre></div><p>The closer similarity value is to 1, the more information is preserved.</p>
<p>Here is a comparison of a resulting similarity with different params.</p>
<pre><code>+-----------------+----------------+------------+
| New matrix size | Resolve method | Similarity |
+-----------------+----------------+------------+
|          500000 | max            |      0.834 |
|          500000 | avg            |      0.858 |
|          750000 | max            |      0.749 |
|          750000 | avg            |      0.789 |
|          800000 | max            |      0.807 |
|          800000 | avg            |      0.837 |
|         1000000 | max            |      0.931 |
|         1000000 | avg            |      0.941 |
+-----------------+----------------+------------+
</code></pre><p>Notice, that similarity of the models with smaller GCD is smaller, just like we expected.</p>
<h3 id="finally">Finally</h3>
<ul>
<li>
<p>We obtained a model which takes only 2Gb of RAM and 94% similar to the original model.</p>
</li>
<li>
<p>Check out the <a href="https://gist.github.com/generall/68fddb87ae1845d6f54c958ed3d0addb">full implementation</a> on my Gist.</p>
</li>
</ul>
</div>

        
        
        

        
        
    
</main>

        <footer>
            <p class="copyright text-muted">© All rights reserved. Powered by <a href="https://gohugo.io">Hugo</a> and <a href="https://github.com/calintat/minimal">Minimal</a>.</p>
        </footer>

        

        
    </body>

</html>

