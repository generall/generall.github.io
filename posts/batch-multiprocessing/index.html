<!DOCTYPE html>
<html lang="en-us">
    <head>
        

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        
<meta name="description" content="Personal blog about Neural Networks, NLP and Software Engineering.">
<meta name="keywords" content="NLP, neural networks, pytorch, telegram, engineering, ML, fasttext, simaes networks, BERT, transformer, word2vec, deploy, allennlp, blog">


<script type="application/ld+json">
  {
      "@context" : "http://schema.org",
      "@type" : "BlogPosting",
      "mainEntityOfPage": {
           "@type": "WebPage",
           "@id": "https:\/\/comprehension.ml\/"
      },
      "articleSection" : "posts",
      "name" : "Parallel preprocessing",
      "headline" : "Parallel preprocessing",
      "description" : "How to prepare batch faster than GPU could process it? How to use advantage of multiple cores properly?",
      "inLanguage" : "en-US",
      "author" : "Andrey Vasnetsov",
      "creator" : "Andrey Vasnetsov",
      "publisher": "Andrey Vasnetsov",
      "accountablePerson" : "Andrey Vasnetsov",
      "copyrightHolder" : "Andrey Vasnetsov",
      "copyrightYear" : "2018",
      "datePublished": "2018-07-24 01:50:06 \u002b0300 \u002b0300",
      "dateModified" : "2018-07-24 01:50:06 \u002b0300 \u002b0300",
      "url" : "https:\/\/comprehension.ml\/posts\/batch-multiprocessing\/",
      "wordCount" : "451",
      "keywords" : [ "NLP", "neural networks", "pytorch", "telegram", "engineering", "ML", "fasttext", "simaes networks", "BERT", "transformer", "word2vec", "deploy", "allennlp", "Blog" ]
  }
</script>

        <title>Parallel preprocessing</title>
        
        <style>

    html body {
        font-family: 'Raleway', sans-serif;
        background-color: white;
    }

    :root {
        --accent: red;
        --border-width:  5px ;
    }

</style>


<link rel="stylesheet" href="https://comprehension.ml/css/main.css">





<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Raleway">


 <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/monokailight.min.css"> 


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
 

    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/scala.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/json.min.js"></script>
    
    <script>hljs.initHighlightingOnLoad();</script>






<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>


<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>


<script>$(document).on('click', function() { $('.collapse').collapse('hide'); })</script>
 <meta name="generator" content="Hugo 0.72.0" />
        

        
            <script async src="https://www.googletagmanager.com/gtag/js?id=UA-56301881-3"></script>
            <script>
              window.dataLayer = window.dataLayer || [];
              function gtag(){dataLayer.push(arguments)};
              gtag('js', new Date());
              gtag('config', 'UA-56301881-3');
            </script>
        

        
            <meta name="yandex-verification" content="6c7b43afecb1426c" />
        

        
            <script type="text/x-mathjax-config">
                MathJax.Hub.Config({
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    processEscapes: true
                }
                });
            </script>
            <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        

        

    </head>

    <body>
        
        <nav class="navbar navbar-default navbar-fixed-top">
            <div class="container">
                <div class="navbar-header">
                    <a class="navbar-brand visible-xs" href="#">Parallel preprocessing</a>
                    <button class="navbar-toggle" data-target=".navbar-collapse" data-toggle="collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                </div>
                <div class="collapse navbar-collapse">
                    
                        <ul class="nav navbar-nav">
                            
                                <li><a href="https://comprehension.ml/">Home</a></li>
                            
                                <li><a href="https://comprehension.ml/andrey_vasnetsov_cv.pdf">CV</a></li>
                            
                                <li><a href="https://comprehension.ml/posts/">Posts</a></li>
                            
                        </ul>
                    
                    
                        <ul class="nav navbar-nav navbar-right">
                            
                                <li class="navbar-icon"><a href="mailto:vasnetsov93&#43;blog@gmail.com"><i class="fa fa-envelope-o"></i></a></li>
                            
                                <li class="navbar-icon"><a href="https://github.com/generall"><i class="fa fa-github"></i></a></li>
                            
                                <li class="navbar-icon"><a href="https://t.me/neural_network_engineering"><i class="fa fa-telegram"></i></a></li>
                            
                        </ul>
                    
                </div>
            </div>
        </nav>


<main>
    
        <div>
            <h2>Parallel preprocessing</h2>
            <h5>July 24, 2018</h5>
            

        </div>

        <div align="start" class="content"><p>Using multiple processes to construct train batches may significantly reduce total training time of your network.
Basically, if you are using GPU for training, you can reduce additional batch construction time almost to zero. This is achieved through pipelining of computations: while GPU crunches numbers, CPU makes preprocessing. Python <code>multiprocessing</code> module allows us to implement such pipelining as elegant as it is possible in the language with GIL.</p>
<p>PyTorch <code>DataLoader</code> class, for example, also uses <code>multiprocessing</code> in it&rsquo;s internals.
Unfortunately <code>DataLoader</code> suffers lack of flexibility. It&rsquo;s impossible to create batch with any complex structure within standard <code>DataLoader</code> class. So it should be useful to be able to apply raw <code>multiprocessing</code>.</p>
<p><code>multiprocessing</code> gives us a set of useful APIs to distribute computations among several processes. Processes does not share memory with each other, so data is transmitted via inter-process communication protocols. For example in linux-like operation systems <code>multiprocessing</code> uses pipes. Such organization leads to some pitfalls that I am going to tell you.</p>
<h2 id="map-vs-imap"><code>map</code> vs <code>imap</code></h2>
<p>Methods <code>map</code> and <code>imap</code> may be used to apply preprocessing to batches. Both of them take processing function and iterable as argument. The difference is that <code>imap</code> is lazy. It will return processed elements as soon as they are ready. In this case all processed batched should not be stored in RAM simultaneously. For training NN you should always prefer <code>imap</code>:</p>
<div class="highlight"><pre style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#00a8c8">def</span> <span style="color:#75af00">process</span><span style="color:#111">(</span><span style="color:#111">batch_reader</span><span style="color:#111">):</span>
    <span style="color:#00a8c8">with</span> <span style="color:#111">Pool</span><span style="color:#111">(</span><span style="color:#111">threads</span><span style="color:#111">)</span> <span style="color:#00a8c8">as</span> <span style="color:#111">pool</span><span style="color:#111">:</span>
        <span style="color:#00a8c8">for</span> <span style="color:#111">batch</span> <span style="color:#f92672">in</span> <span style="color:#111">pool</span><span style="color:#f92672">.</span><span style="color:#111">imap</span><span style="color:#111">(</span><span style="color:#111">foo</span><span style="color:#111">,</span> <span style="color:#111">batch_reader</span><span style="color:#111">):</span>
            <span style="color:#f92672">....</span>
            <span style="color:#00a8c8">yield</span> <span style="color:#111">batch</span>
            <span style="color:#f92672">....</span>
</code></pre></div><h2 id="serialization">Serialization</h2>
<p>Other pitfall is associated with the need to transfer objects via pipes. In addition to the processing results, <code>multiprocessing</code> will also serialize transformation object if it is used like this: <code>pool.imap(transformer.foo, batch_reader)</code>. <code>transformer</code> will be serialized and send to subprocess. It may lead to some problems if <code>transformer</code> object has large properties. In this case it may be better to store large properties as singleton class variables:</p>
<div class="highlight"><pre style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#00a8c8">class</span> <span style="color:#75af00">Transformer</span><span style="color:#111">():</span>
    <span style="color:#111">large_dictinary</span> <span style="color:#f92672">=</span> <span style="color:#111">None</span>

    <span style="color:#00a8c8">def</span> <span style="color:#111">__init__</span><span style="color:#111">(</span><span style="color:#111">self</span><span style="color:#111">,</span> <span style="color:#111">large_dictinary</span><span style="color:#111">,</span> <span style="color:#f92672">**</span><span style="color:#111">kwargs</span><span style="color:#111">):</span>
        <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">__class__</span><span style="color:#f92672">.</span><span style="color:#111">large_dictinary</span> <span style="color:#f92672">=</span> <span style="color:#111">large_dictinary</span>

    <span style="color:#00a8c8">def</span> <span style="color:#75af00">foo</span><span style="color:#111">(</span><span style="color:#111">self</span><span style="color:#111">,</span> <span style="color:#111">x</span><span style="color:#111">):</span>
        <span style="color:#f92672">....</span>
        <span style="color:#111">y</span> <span style="color:#f92672">=</span> <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">large_dictinary</span><span style="color:#111">[</span><span style="color:#111">x</span><span style="color:#111">]</span>
        <span style="color:#f92672">....</span>
</code></pre></div><p>Another difficulty that you may encounter is if the preprocessor is faster than GPU learning. In this case unprocessed batches accumulate in memory. If your memory is not to large enough you will get Out-of-Memory error. One way to solve this problem is to limit batch preprocessing until GPU learning is done.
<code>Semaphore</code> is perfect solution for this task:</p>
<div class="highlight"><pre style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#00a8c8">def</span> <span style="color:#75af00">batch_reader</span><span style="color:#111">(</span><span style="color:#111">semaphore</span><span style="color:#111">):</span>
    <span style="color:#00a8c8">for</span> <span style="color:#111">batch</span> <span style="color:#f92672">in</span> <span style="color:#111">source</span><span style="color:#111">:</span>
        <span style="color:#111">semaphore</span><span style="color:#f92672">.</span><span style="color:#111">acquire</span><span style="color:#111">()</span>
        <span style="color:#00a8c8">yield</span> <span style="color:#111">batch</span>


<span style="color:#00a8c8">def</span> <span style="color:#75af00">process</span><span style="color:#111">(</span><span style="color:#111">x</span><span style="color:#111">):</span>
    <span style="color:#00a8c8">return</span> <span style="color:#111">x</span> <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>


<span style="color:#00a8c8">def</span> <span style="color:#75af00">pooling</span><span style="color:#111">():</span>
    <span style="color:#00a8c8">with</span> <span style="color:#111">Pool</span><span style="color:#111">(</span><span style="color:#111">threads</span><span style="color:#111">)</span> <span style="color:#00a8c8">as</span> <span style="color:#111">pool</span><span style="color:#111">:</span>
        <span style="color:#111">semaphore</span> <span style="color:#f92672">=</span> <span style="color:#111">Semaphore</span><span style="color:#111">(</span><span style="color:#111">limit</span><span style="color:#111">)</span>
        <span style="color:#00a8c8">for</span> <span style="color:#111">x</span> <span style="color:#f92672">in</span> <span style="color:#111">pool</span><span style="color:#f92672">.</span><span style="color:#111">imap</span><span style="color:#111">(</span><span style="color:#111">plus</span><span style="color:#111">,</span> <span style="color:#111">batch_reader</span><span style="color:#111">(</span><span style="color:#111">semaphore</span><span style="color:#111">)):</span>
            <span style="color:#00a8c8">yield</span> <span style="color:#111">x</span>
            <span style="color:#111">semaphore</span><span style="color:#f92672">.</span><span style="color:#111">release</span><span style="color:#111">()</span>


<span style="color:#00a8c8">for</span> <span style="color:#111">x</span> <span style="color:#f92672">in</span> <span style="color:#111">pooling</span><span style="color:#111">():</span>
    <span style="color:#111">learn_gpu</span><span style="color:#111">(</span><span style="color:#111">x</span><span style="color:#111">)</span>
</code></pre></div><p><code>Semaphore</code> has internal counter syncronized across all working processes. It&rsquo;s logic will block execution if some process tries to increase counet value above limit with <code>semaphore.acquire()</code></p>
</div>

        
        
        

        
        
    
</main>

        <footer>
            <p class="copyright text-muted">© All rights reserved. Powered by <a href="https://gohugo.io">Hugo</a> and <a href="https://github.com/calintat/minimal">Minimal</a>.</p>
        </footer>

        

        
    </body>

</html>

